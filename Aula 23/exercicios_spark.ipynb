{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e37c0df-e67b-449d-8048-3be19047ab6d",
   "metadata": {},
   "source": [
    "# Exercicios com Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a15e8-fd4b-468c-9b7e-d9a4e56baf3a",
   "metadata": {},
   "source": [
    "## Instalando o ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f41a9-c638-4e0f-9493-77654a650474",
   "metadata": {},
   "source": [
    "O jeito mais simples de começar a trabalhar com Spark é instalar um container com tudo pronto! No site https://hub.docker.com/r/jupyter/pyspark-notebook vemos uma imagem Docker que já vem com `pyspark` e `jupyter lab`. Instale a imagem com o comando:\n",
    "\n",
    "```bash\n",
    "docker pull jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "Vamos iniciar o ambiente de trabalho com o comando `docker run`. Para isso precisamos tomar alguns cuidados:\n",
    "\n",
    "1) Temos que mapear nosso diretorio local de trabalho para um diretório interno do container, de modo que alterações feitas dentro do container (nesta pasta escolhida) sejam gravadas no nosso diretorio local. No container temos um usuário padrão com *username* `jovyan`. No *homedir* desse usuario temos uma pasta vazia `work`, que vai servir como local de mapeamento do nosso diretorio local de trabalho. Podemos então fazer esse mapeamendo com a opção `-v` do comando `docker run` da seguinte forma:\n",
    "\n",
    "```bash\n",
    "-v <diretorio>:/home/jovyan/work\n",
    "```\n",
    "\n",
    "onde `<diretorio>` representa seu diretorio local de trabalho.\n",
    "\n",
    "2) Para acessar o `jupyter notebook` e o *dashboard* do Spark a partir do nosso *browser* favorito temos que abrir algumas portas do container com a opção `-p`. As portas são `8888` (para o próprio `jupyter notebook`) e `4040` (para o *dashboard* do Spark). Ou seja, adicionaremos às opções do `docker run`o seguinte:\n",
    "\n",
    "```bash\n",
    "-p 8888:8888 -p 4040:4040\n",
    "```\n",
    "\n",
    "Desta forma, ao acessar `localhost:8888` na nossa máquina, estaremos acessando o servidor Jupyter na porta 8888 interna do container.\n",
    "\n",
    "3) Vamos iniciar o container no modo interativo, e vamos especificar que o container deve ser encerrado ao fechar o servidor Jupyter. Faremos isso com as opções `-it` e `-rm`\n",
    "\n",
    "Portanto, o comando completo que eu uso na minha máquina Linux para iniciar o container é:\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "    -it \\\n",
    "    --rm \\\n",
    "    -p 8888:8888 \\\n",
    "    -p 4040:4040 \\\n",
    "    -v `pwd`:/home/jovyan/work \\\n",
    "    jupyter/pyspark-notebook\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Para facilitar a vida eu coloco esse comando em um arquivo `inicia.sh`. Engenheiros, façam do jeito que preferirem!\n",
    "\n",
    "Agora abra esse notebook lá no container!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7292d9-4aa8-47c3-938b-3922cd4be036",
   "metadata": {},
   "source": [
    "## Iniciando o Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353630ab-3906-495b-9c46-741f4f328bf1",
   "metadata": {},
   "source": [
    "Vamos iniciar o ambiente Spark. Para isso vamos:\n",
    "\n",
    "1) Criar um objeto de configuração do ambiente Spark. Nossa configuração será simples: vamos especificar que o nome da nossa aplicação Spark é \"Minha aplicação\", e que o *master node* é a máquina local, usando todos os *cores* disponíveis. Aplicações reais de Spark são configuradas de modo ligeiramente diferente: ao especificar o *master node* passamos uma URL real, com o endereço do nó gerente do *cluster* Spark.\n",
    "\n",
    "2) Vamos criar um objeto do tipo `SparkContext` com essa configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c208f2-a1ef-4f30-bf7c-5ed01e20246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setAppName('Minha aplicação')\n",
    "conf.setMaster('local[*]')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a5b288-9ccf-45c1-98d9-48e0f4c308d8",
   "metadata": {},
   "source": [
    "O `SparkContext` é a nossa porta de entrada para o cluster Spark, ele será a raiz de todas as nossas operações com o Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e478c-cfe7-4bca-8ebf-6c34dd38aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d273e0bd-9e09-46b9-b828-fb61e100a9f9",
   "metadata": {},
   "source": [
    "O link acima provavelmente não funcionará porque ele se refere à porta 4040 interna do container (portanto a URL está com endereço interno). Porém fizemos o mapeamento da porta 4040 interna para a porta 4040 externa, logo você pode acessar o *dashboard* do Spark no endereço http://localhost:4040\n",
    "\n",
    "<center><img src=\"./spark_dashboard.png\" width=800/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3099ef4-2117-47a8-8a1d-af2df7814056",
   "metadata": {},
   "source": [
    "## Trabalhando com Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b1de3-9e55-46ce-99e8-526b05b8c3d2",
   "metadata": {},
   "source": [
    "Para este exercicio vamos trabalhar com o dataset de reviews da Amazon visto em https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews. Baixe o arquivo \"train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65caf38e-fbd8-4513-af02-37671c6a949b",
   "metadata": {},
   "source": [
    "Vamos ler o arquivo \"train.csv\" em um RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ec721-3b7f-46b8-a8a9-d27ccc83f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73e675-ea3b-49c2-8622-53e8c17cfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4794fc0-59cf-497f-98fc-d20efc0cca2b",
   "metadata": {},
   "source": [
    "De acordo com a documentação deste arquivo vista no Kaggle, cada linha contem 2 elementos: o sentimento do review (1 - negativo, 2 - positivo), o título e o corpo do review. A linha contem esses elementos em um formato \"comma-separated value\" (CSV), onde cada um dos campos está delimitado por aspas duplas. Se o texto em si (titulo ou corpo) contem aspas, elas aparecem como um par de aspas duplas. Vamos usar o `.filter()` para achar um exemplo desses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c736d-0378-43f7-be05-3e7f53b1423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_line = rdd.filter(lambda x: '\"\"' in x).take(1)\n",
    "example_line = example_line[0]\n",
    "\n",
    "example_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0e86d-122f-4c0e-bf36-a0a8e2f60559",
   "metadata": {},
   "source": [
    "Levando isso em consideração, vamos fazer uma função simples para separar os campos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923aa8b-1bee-4c74-8d18-6f4cf0445b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    parts = line[1:-1].split('\",\"')\n",
    "    sentiment = int(parts[0])\n",
    "    title = parts[1].replace('\"\"', '\"')\n",
    "    body = parts[2].replace('\"\"', '\"')\n",
    "    return (sentiment, title, body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f380c6-469c-4dd2-baa5-cbe72110de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_line(example_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8249e3-6f4a-4ed0-81b5-a453d141b88b",
   "metadata": {},
   "source": [
    "Podemos agora utilizar nossa função para separar os campos de cada linha do dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d103d0-ae4d-4912-ba1b-8133f12d4e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_split = rdd.map(parse_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64fbea0-1378-4be7-b1d7-ada7828735b4",
   "metadata": {},
   "source": [
    "Como de costume, nada realmente acontece até que uma \"action\" seja invocada. O `.map()` é uma \"transformation\". Vamos usar uma action simples para \"materializar\" o novo RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b670e5-1757-443b-a229-453e3e3115b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_split.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff144ad-9378-4b72-ae84-1b1d2dda815d",
   "metadata": {},
   "source": [
    "Vamos explorar os resultados para ver se deu certo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7882d9-4502-406b-abc4-b74de6fa1aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_split.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3317a957-f13d-49fe-b193-9c6826616cc7",
   "metadata": {},
   "source": [
    "**Atividade**: Conte quantos sentimentos diferentes existem, e quantas vezes aparecem, para confirmar que só tem os sentimentos 1 e 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e5d27-5687-419b-94f0-0d7a40ef62b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fffe0feb-e926-485d-80c0-897bf7820e47",
   "metadata": {},
   "source": [
    "**Atividade**: Quantos reviews não tem titulo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459334bc-f993-4e73-97e0-377899cae44c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a31da43c-d2a5-4806-a5bf-9765e7fb09f9",
   "metadata": {},
   "source": [
    "**Atividade**: Quantos reviews não tem corpo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79e04a-1d9b-458e-b4b5-62b8ee219659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c76d3fd0-af72-4b2e-aa61-7ffc897b86c1",
   "metadata": {},
   "source": [
    "**Atividade**: Qual o comprimento máximo de um título e de um corpo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c89e59-40b4-4fa1-bef2-8d406044cc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11d96887-6d6d-494e-9ac0-0c06b5c08dc6",
   "metadata": {},
   "source": [
    "**Atividade**: Qual a maior palavra palíndroma no titulo ou corpo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851b744-9826-4de8-83a1-177574b53123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d63f641d-1d98-4d84-8f7b-93b122ae057c",
   "metadata": {},
   "source": [
    "**Atividade**: Quais as 20 palavras mais populares do titulo? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de29253-98bc-4c5f-b3fe-fc35bee98503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
